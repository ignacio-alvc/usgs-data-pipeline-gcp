{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Clonación del repositorio***"
      ],
      "metadata": {
        "id": "JmuuZguf2AAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ignacio-alvc/usgs-data-pipeline-gcp.git"
      ],
      "metadata": {
        "id": "PU_00lcvxjCF",
        "outputId": "f8bd0cb3-8487-402a-8c1a-a53f3bd61dce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'usgs-data-pipeline-gcp' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Instalación de librerías***"
      ],
      "metadata": {
        "id": "SFO6K9Rb0pDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost google-cloud-bigquery[pandas] joblib google-cloud-storage scikit-learn pandas"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wCdOusGpx1vz",
        "outputId": "9b88bee6-a724-4211-a686-0ae24dd74463",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: google-cloud-bigquery[pandas] in /usr/local/lib/python3.12/dist-packages (3.38.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.2)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=2.11.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery[pandas]) (2.26.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery[pandas]) (2.38.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery[pandas]) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery[pandas]) (2.7.2)\n",
            "Requirement already satisfied: packaging>=24.2.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery[pandas]) (25.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery[pandas]) (2.9.0.post0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery[pandas]) (2.32.4)\n",
            "Requirement already satisfied: pandas-gbq>=0.26.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery[pandas]) (0.29.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.47.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery[pandas]) (1.75.1)\n",
            "Requirement already satisfied: pyarrow>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery[pandas]) (18.1.0)\n",
            "Requirement already satisfied: db-dtypes<2.0.0,>=1.0.4 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery[pandas]) (1.4.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (1.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery[pandas]) (1.71.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery[pandas]) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery[pandas]) (1.26.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery[pandas]) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery[pandas]) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery[pandas]) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery[pandas]) (4.9.1)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio<2.0.0,>=1.47.0->google-cloud-bigquery[pandas]) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from pandas-gbq>=0.26.1->google-cloud-bigquery[pandas]) (75.2.0)\n",
            "Requirement already satisfied: pydata-google-auth>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from pandas-gbq>=0.26.1->google-cloud-bigquery[pandas]) (1.9.1)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from pandas-gbq>=0.26.1->google-cloud-bigquery[pandas]) (1.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery[pandas]) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery[pandas]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery[pandas]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery[pandas]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery[pandas]) (2025.10.5)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from google-auth-oauthlib>=0.7.0->pandas-gbq>=0.26.1->google-cloud-bigquery[pandas]) (2.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-bigquery[pandas]) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.7.0->pandas-gbq>=0.26.1->google-cloud-bigquery[pandas]) (3.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Importaciones***"
      ],
      "metadata": {
        "id": "84T1rFNSnSuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta, timezone\n",
        "import os"
      ],
      "metadata": {
        "id": "P48CWRC0nSZO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GCP_CREDENTIALS_FILENAME = \"portfolio-earthquake-analysis-6ff1f9179260.json\"\n",
        "GCP_CREDENTIALS_PATH = f\"/content/{GCP_CREDENTIALS_FILENAME}\"\n",
        "REPO_NAME = \"usgs-data-pipeline-gcp\"\n",
        "REPO_PATH = f\"/content/{REPO_NAME}\"\n",
        "MODEL_DIR = os.path.join(REPO_PATH, \"models\")\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Entorno listo. Repositorio clonado y librerías instaladas.\")\n",
        "print(f\"Los modelos se guardaran en: {MODEL_DIR}\")"
      ],
      "metadata": {
        "id": "OMdusz7pnx5K",
        "outputId": "0f32d969-6365-4753-ab78-910e6979f5ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entorno listo. Repositorio clonado y librerías instaladas.\n",
            "Los modelos se guardaran en: /content/usgs-data-pipeline-gcp/models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "end_time = datetime.now(timezone.utc)\n",
        "start_time = end_time - timedelta(days=180)\n",
        "start_str = start_time.strftime('%Y-%m-%dT%H:%M:%S')\n",
        "end_str = end_time.strftime('%Y-%m-%dT%H:%M:%S')\n",
        "\n",
        "api_url = f\"https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={start_str}&endtime={end_str}&minmagnitude=2.5\"\n",
        "\n",
        "print(f\"Descargando datos desde {start_time.date()} hasta {end_time.date()}...\")\n",
        "\n",
        "try:\n",
        "    response = requests.get(api_url)\n",
        "    response.raise_for_status()\n",
        "    data = response.json()\n",
        "    features = data.get('features', [])\n",
        "\n",
        "    if not features:\n",
        "        print(\"No se encontraron sismos.\")\n",
        "        df = pd.DataFrame()\n",
        "    else:\n",
        "        records = []\n",
        "        for feature in features:\n",
        "            props = feature.get('properties', {})\n",
        "            geom = feature.get('geometry', {})\n",
        "            coords = geom.get('coordinates', [None, None, None])\n",
        "            if props.get('mag') is not None and len(coords) == 3 and coords[0] is not None and coords[1] is not None and coords[2] is not None:\n",
        "                 records.append({\n",
        "                     'magnitude': props.get('mag'),\n",
        "                     'latitude': coords[1],\n",
        "                     'longitude': coords[0],\n",
        "                     'depth_km': coords[2],\n",
        "                 })\n",
        "        df = pd.DataFrame(records)\n",
        "        df.dropna(inplace=True)\n",
        "\n",
        "    print(f\"Se procesaron {len(df)} sismos (mag >= 2.5).\")\n",
        "    df.head()\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error al descargar datos: {e}\")\n",
        "    df = pd.DataFrame()\n",
        "if not os.path.exists(GCP_CREDENTIALS_PATH):\n",
        "    print(f\"ERROR: No se encontró el archivo de credenciales en {GCP_CREDENTIALS_PATH}. ¿Lo subiste?\")\n",
        "else:\n",
        "    print(\"Archivo de credenciales encontrado.\")"
      ],
      "metadata": {
        "id": "_KsN1SEGl69l",
        "outputId": "329b950e-4ad0-49ae-f5d4-5d2ebe724e3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargando datos desde 2025-04-28 hasta 2025-10-25...\n",
            "Se procesaron 14403 sismos (mag >= 2.5).\n",
            "Archivo de credenciales encontrado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Título: Preprocesamiento y División de Datos ---\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "if not df.empty:\n",
        "    # 1. Separar características (X) y objetivo (y)\n",
        "    features = ['latitude', 'longitude', 'depth_km']\n",
        "    target = 'magnitude'\n",
        "    X = df[features]\n",
        "    y = df[target]\n",
        "\n",
        "    # 2. Dividir en entrenamiento y prueba\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # 3. Escalar características\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # 4. Guardar el escalador\n",
        "    SCALER_FILENAME = os.path.join(MODEL_DIR, \"earthquake_scaler.joblib\")\n",
        "    joblib.dump(scaler, SCALER_FILENAME)\n",
        "\n",
        "    print(\"Preprocesamiento completado. Escalador guardado.\")\n",
        "    print(f\"Scaler: {SCALER_FILENAME}\")\n",
        "    # Verificar que el archivo existe\n",
        "    !ls -lh {MODEL_DIR}\n",
        "else:\n",
        "    print(\"El DataFrame está vacío. No se puede continuar con el preprocesamiento.\")"
      ],
      "metadata": {
        "id": "PJiAaIbMrOa0",
        "outputId": "0f725827-a170-42a6-fcb2-a0cf6d81aaa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocesamiento completado. Escalador guardado.\n",
            "Scaler: /content/usgs-data-pipeline-gcp/models/earthquake_scaler.joblib\n",
            "total 4.0K\n",
            "-rw-r--r-- 1 root root 959 Oct 25 18:41 earthquake_scaler.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Título: Entrenamiento del Modelo (XGBoost) ---\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "if not df.empty:\n",
        "    print(\"Iniciando entrenamiento con XGBoost y GridSearchCV...\")\n",
        "\n",
        "    xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200],\n",
        "        'learning_rate': [0.1, 0.05],\n",
        "        'max_depth': [3, 5]\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=xgb_reg,\n",
        "        param_grid=param_grid,\n",
        "        cv=5, # 5-fold cross-validation\n",
        "        scoring='neg_mean_squared_error',\n",
        "        verbose=1,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X_train_scaled, y_train)\n",
        "    best_xgb_model = grid_search.best_estimator_\n",
        "\n",
        "    print(\"\\nMejores hiperparámetros:\")\n",
        "    print(grid_search.best_params_)\n",
        "\n",
        "    # Evaluar\n",
        "    y_pred = best_xgb_model.predict(X_test_scaled)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    print(f\"\\nRMSE en prueba: {rmse:.4f}\")\n",
        "\n",
        "    # Guardar modelo\n",
        "    MODEL_FILENAME = os.path.join(MODEL_DIR, \"earthquake_xgboost_model.joblib\")\n",
        "    joblib.dump(best_xgb_model, MODEL_FILENAME)\n",
        "    print(f\"Modelo XGBoost guardado en: {MODEL_FILENAME}\")\n",
        "    # Verificar que el archivo existe\n",
        "    !ls -lh {MODEL_DIR}\n",
        "\n",
        "else:\n",
        "     print(\"El DataFrame está vacío. No se puede entrenar el modelo.\")"
      ],
      "metadata": {
        "id": "cskY8Na0rR9G",
        "outputId": "637813b3-d7a0-48f5-d394-f15a1db45e09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando entrenamiento con XGBoost y GridSearchCV...\n",
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "\n",
            "Mejores hiperparámetros:\n",
            "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}\n",
            "\n",
            "RMSE en prueba: 0.4214\n",
            "Modelo XGBoost guardado en: /content/usgs-data-pipeline-gcp/models/earthquake_xgboost_model.joblib\n",
            "total 256K\n",
            "-rw-r--r-- 1 root root  959 Oct 25 18:41 earthquake_scaler.joblib\n",
            "-rw-r--r-- 1 root root 252K Oct 25 18:42 earthquake_xgboost_model.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Definición de variables***"
      ],
      "metadata": {
        "id": "3El9zk7nzUVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuración**"
      ],
      "metadata": {
        "id": "Z2qni1C70BGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Carga de datos desde BigQuery**"
      ],
      "metadata": {
        "id": "TVSBYLSs16C1"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}